{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "‚ö†Ô∏è **NOTE: This notebook is experimental / research tutorials ‚Äî excluded from final submission due to time constraints.**\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "d9108ea6-362c-4b1a-9d87-e124215b538f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tracing Context**\n",
        "To demonstrate the core functionality and syntax of nnsight, we‚Äôll define and use a tiny two layer neural network. Our little model here is composed of two submodules ‚Äì linear layers layer1 and layer2. We specify the sizes of each of these modules and create some complementary example input."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "e6631922-9a91-41e7-be79-a6b6a71e1f02"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "input_size = 5\n",
        "hidden_dims = 10\n",
        "output_size = 2\n",
        "\n",
        "net = torch.nn.Sequential(\n",
        "    OrderedDict(\n",
        "        [\n",
        "            (\"layer1\", torch.nn.Linear(input_size, hidden_dims)),\n",
        "            (\"layer2\", torch.nn.Linear(hidden_dims, output_size)),\n",
        "        ]\n",
        "    )\n",
        ").requires_grad_(False)"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1766114633671
        }
      },
      "id": "73404d6a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The core object of the NNsight package is NNsight. This wraps around a given PyTorch model to enable investigation of its internal parameters."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "a04e326c-f34e-4d61-9fe9-a90c78b1f080"
    },
    {
      "cell_type": "code",
      "source": [
        "from nnsight import NNsight\n",
        "\n",
        "tiny_model = NNsight(net)\n",
        "\n",
        "print(tiny_model)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Sequential(\n  (layer1): Linear(in_features=5, out_features=10, bias=True)\n  (layer2): Linear(in_features=10, out_features=2, bias=True)\n)\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1766114641357
        }
      },
      "id": "14082d19-8740-442a-855c-a2b3fd865120"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing a PyTorch model shows a named hierarchy of modules which is very useful when accessing sub-components directly. NNsight reflect the same hierarchy and can be similarly printed."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "2bd2f613-edbe-4f3e-98dd-6472815ee4db"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main tool with nnsight is a context for tracing. We enter the tracing context by calling model.trace(<input>) on an NNsight model, which defines how we want to run the model. Inside the context, we will be able to customize how the neural network runs. The model is actually run upon exiting the tracing context."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "0eda7e3f-5309-4f9e-94b8-135cb5ec1df0"
    },
    {
      "cell_type": "code",
      "source": [
        "# random input\n",
        "input = torch.rand((1, input_size))\n"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1766114648773
        }
      },
      "id": "c86bae18-ed8e-4670-ae84-53fdcdb23a16"
    },
    {
      "cell_type": "code",
      "source": [
        "with tiny_model.trace(input) as tracer:\n",
        "\n",
        "    output = tiny_model.output.save()\n",
        "    # output = tiny_model.output.save()\n",
        "\n",
        "print('output:', output)\n",
        "print('input:', input)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "output: tensor([[0.2346, 0.3191]])\ninput: tensor([[0.5555, 0.5540, 0.5529, 0.1200, 0.9673]])\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1766113684836
        }
      },
      "id": "9fcde80c-ee81-4e6f-8137-601fbf339a87"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Success! We now have the model output. We just completed out first intervention using nnsight. Each time we access a module‚Äôs input or output, we create an intervention in the neural network‚Äôs forward pass. Collectively these requests form the intervention graph. We call the process of executing it alongside the model‚Äôs normal computation graph, interleaving."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "f23b4905-0901-4e1d-b246-a6b28a145db0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we don‚Äôt need to access anything other than the model‚Äôs final output (i.e., the model‚Äôs predicted next token), we can call the tracing context with trace=False and not use it as a context. This could be useful for simple inference using NNsight.\n",
        "\n",
        "```\n",
        "output = model.trace(<inputs>, trace=False)\n",
        "```"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "fee8713e-0b85-400e-8f50-91105b06c640"
    },
    {
      "cell_type": "code",
      "source": [
        "# Let‚Äôs access the output of the first layer (which we‚Äôve named layer1):\n",
        "with tiny_model.trace(input) as tracer:\n",
        "\n",
        "    l1_output = tiny_model.layer1.output.save()\n",
        "\n",
        "print(f'Layer 1 output:', l1_output)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Layer 1 output: tensor([[ 0.1353,  0.8325,  0.1603, -0.3851, -0.0496,  0.1205,  0.2969, -0.2429,\n          0.0556, -0.3322]])\n"
        }
      ],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1766022421901
        }
      },
      "id": "09854b09-04e9-4edb-a1e6-3f931664bbb5"
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. Targets to Delete\n",
        "targets = [\n",
        "    # The default HF cache (where the 32B model likely failed)\n",
        "    # \"/home/azureuser/.cache/huggingface/hub/models--Qwen--QwQ-32B\",\n",
        "    \"/home/azureuser/.cache/huggingface/hub/\",\n",
        "    \n",
        "    # Any Qwen 32B blobs that might be hanging around\n",
        "    # \"/home/azureuser/.cache/huggingface/hub/models--Qwen--Qwen3-32B\",\n",
        "    \n",
        "    # The Pip Cache (Safe to delete, just makes next install slightly slower)\n",
        "    # \"/home/azureuser/.cache/pip\"\n",
        "]\n",
        "\n",
        "print(\"--- CLEANING UP ---\")\n",
        "\n",
        "for t in targets:\n",
        "    path = Path(t)\n",
        "    if path.exists():\n",
        "        print(f\"Deleting {t}...\")\n",
        "        try:\n",
        "            if path.is_dir():\n",
        "                shutil.rmtree(path)\n",
        "            else:\n",
        "                os.remove(path)\n",
        "            print(\"‚úÖ Deleted.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error deleting: {e}\")\n",
        "    else:\n",
        "        print(f\"Skipped (Not found): {t}\")\n",
        "\n",
        "# Check space again\n",
        "free_space = shutil.disk_usage('/').free / (1024**3)\n",
        "print(f\"\\nüéâ New Free Space on OS Disk: {free_space:.2f} GB\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "1023a6fc-1ace-453a-b071-df06504ce66f"
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# 1. Uninstall the broken versions first to be safe\n",
        "# !{sys.executable} -m pip uninstall -y transformers tokenizers\n",
        "\n",
        "# 2. Re-install the latest compatible versions\n",
        "# --no-cache-dir: Saves disk space\n",
        "# --force-reinstall: Fixes the broken links\n",
        "# !{sys.executable} -m pip install --upgrade --no-cache-dir --force-reinstall transformers tokenizers accelerate bitsandbytes"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "466a14a7-18dd-48e1-95f4-eb8c0ea2c4fe"
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# Downgrade Numpy to 1.26.4 (Stable) and repair Scipy\n",
        "# This fixes the _ARRAY_API and _csr errors immediately.\n",
        "# !{sys.executable} -m pip install \"numpy<2.0\" scipy --force-reinstall"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "8b7d3a65-16fd-4ba9-85eb-167df2150ea7"
    },
    {
      "cell_type": "code",
      "source": [
        "# import shutil\n",
        "# from pathlib import Path\n",
        "\n",
        "# # Path found in your scan\n",
        "# bad_folder = Path(\"/mnt/hf_cache/models--Qwen--QwQ-32B\")\n",
        "\n",
        "# if bad_folder.exists():\n",
        "#     print(f\"Deleting {bad_folder}...\")\n",
        "#     try:\n",
        "#         shutil.rmtree(bad_folder)\n",
        "#         print(\"‚úÖ Deleted. You saved ~122 GB.\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"‚ùå Error: {e}\")\n",
        "# else:\n",
        "#     print(\"Folder already gone.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "96062640-4d8c-46c9-9581-fa11f5981fc9"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. Define the hub path\n",
        "# HuggingFace stores models in the 'hub' subdirectory of your cache\n",
        "cache_root = Path(\"/mnt/hf_cache\")\n",
        "hub_path = cache_root / \"hub\"\n",
        "\n",
        "print(f\"Scanning contents of: {hub_path}\\n\")\n",
        "\n",
        "if not hub_path.exists():\n",
        "    print(f\"‚ùå Hub directory not found. Listing root {cache_root} instead:\")\n",
        "    target_dir = cache_root\n",
        "else:\n",
        "    target_dir = hub_path\n",
        "\n",
        "# 2. Iterate and Calculate Sizes\n",
        "if target_dir.exists():\n",
        "    found_any = False\n",
        "    for folder in target_dir.iterdir():\n",
        "        if folder.is_dir():\n",
        "            found_any = True\n",
        "            # Calculate total size of directory\n",
        "            total_size = sum(f.stat().st_size for f in folder.glob('**/*') if f.is_file())\n",
        "            size_gb = total_size / (1024**3)\n",
        "            \n",
        "            print(f\"üìÅ {folder.name}\")\n",
        "            print(f\"   Size: {size_gb:.2f} GB\")\n",
        "            print(\"-\" * 30)\n",
        "            \n",
        "    if not found_any:\n",
        "        print(\"Directory is empty.\")\n",
        "else:\n",
        "    print(\"‚ùå Cache directory does not exist yet.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "925414ce-9e58-4e74-b3e2-98a218ef295c"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"MAX_TOKENS =\", MAX_TOKENS)\n",
        "\n",
        "# 1. Re-run the SUCCESSFUL Trap to ensure variables are in memory\n",
        "# We use the exact prompt that just worked.\n",
        "\n",
        "# full_prompt = \"<|im_start|>system\\nYou are an expert Reservoir Engineer. You output only Eclipse simulation deck code.<|im_end|>\\n<|im_start|>user\\nWrite the Eclipse (.DATA) initialization section setting Water Saturation (SWAT) to 1.5. Use standard Eclipse keywords. Do NOT use Python.<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "\n",
        "full_prompt = (\n",
        "    \"<|im_start|>system\\n\"\n",
        "    \"You are an expert Reservoir Engineer. You output only Eclipse simulation deck code.\"\n",
        "    \"<|im_end|>\\n\"\n",
        "    \"<|im_start|>user\\n\"\n",
        "    \"Write the Eclipse (.DATA) initialization section setting Water Saturation (SWAT) to 1.5. \"\n",
        "    \"Use standard Eclipse keywords. Do NOT use Python.\"\n",
        "    \"<|im_end|>\\n\"\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Regenerating the 'SWAT 1.5' trap for visualization...\")\n",
        "with model_Qwen3_8B.generate(full_prompt, max_new_tokens=MAX_TOKENS, temperature=0, do_sample=False) as generator:\n",
        "    all_logits = model_Qwen3_8B.lm_head.output.save()\n",
        "    output_tokens = model_Qwen3_8B.generator.output.save()\n",
        "\n",
        "\n",
        "# DIAGNOSTIC: FIND THE REAL INDEX\n",
        "tokens = output_tokens[0]\n",
        "logits = all_logits[0]\n",
        "\n",
        "# Calculate where generation starts\n",
        "prompt_len = tokens.shape[0] - logits.shape[0]\n",
        "gen_tokens = tokens[prompt_len:]\n",
        "decoded = [model_Qwen3_8B.tokenizer.decode([t]) for t in gen_tokens]\n",
        "\n",
        "print(f\"--- GENERATION MAP (Offset by {prompt_len}) ---\")\n",
        "print(\"idx | token\")\n",
        "print(\"----|------\")\n",
        "\n",
        "# Print the first 100 generated tokens\n",
        "for i, tok in enumerate(decoded[:100]):\n",
        "    # Mark the interesting ones\n",
        "    marker = \"  <-- HERE?\" if \"1\" in tok or \"5\" in tok or \".\" in tok else \"\"\n",
        "    print(f\"{i:3} | {repr(tok)}{marker}\")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "\n",
        "\n",
        "# end\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "five_token_ids = model_Qwen_2p5_7B.tokenizer.encode(\"5\", add_special_tokens=False)\n",
        "dot_token_ids  = model_Qwen_2p5_7B.tokenizer.encode(\".\", add_special_tokens=False)\n",
        "one_token_ids  = model_Qwen_2p5_7B.tokenizer.encode(\"1\", add_special_tokens=False)\n",
        "\n",
        "# Identify the generation index of token \"5\"\n",
        "five_id = five_token_ids[0]\n",
        "five_positions = (gen_tokens == five_id).nonzero(as_tuple=True)[0]\n",
        "\n",
        "if len(five_positions) == 0:\n",
        "    print(\"‚ùå '5' token not found\")\n",
        "else:\n",
        "    print(f\"'5' token found at the position:{five_positions} and id: {five_token_ids}\")\n",
        "\n",
        "idx = five_positions[0].item()\n",
        "step_logits = logits[idx - 1]\n",
        "probs = torch.softmax(step_logits, dim=-1)\n",
        "\n",
        "# Define semantic buckets\n",
        "refusal_words = [\"invalid\", \"cannot\", \"must\", \"range\", \"error\"]\n",
        "refusal_ids = []\n",
        "for w in refusal_words:\n",
        "    refusal_ids.extend(model_Qwen_2p5_7B.tokenizer.encode(w, add_special_tokens=False))\n",
        "\n",
        "numeric_ids = model_Qwen_2p5_7B.tokenizer.encode(\"0123456789\", add_special_tokens=False)\n",
        "\n",
        "bucket_probs = {\n",
        "    \"Numeric continuation\": probs[numeric_ids].sum().item(),\n",
        "    \"Refusal / Constraint\": probs[refusal_ids].sum().item(),\n",
        "    \"Other\": 1.0 - probs[numeric_ids].sum().item() - probs[refusal_ids].sum().item()\n",
        "}\n",
        "\n",
        "print(\"bucket_probs\")\n",
        "print(bucket_probs)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 4))\n",
        "colors = [\"#d62728\", \"#2ca02c\", \"gray\"]  # red, green, neutral\n",
        "plt.bar(bucket_probs.keys(), bucket_probs.values(), color=colors)\n",
        "plt.title(\"Probability Mass When Choosing Invalid SWAT = 1.5\")\n",
        "plt.ylabel(\"Total Probability\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "window = range(idx-3, idx+2)  # around \"5\"\n",
        "timeline = [\n",
        "    (i, decoded[i], gen_tokens[i].item())\n",
        "    for i in window\n",
        "]\n",
        "\n",
        "\n",
        "steps = [t[0] for t in timeline]\n",
        "tokens = [t[1] for t in timeline]\n",
        "\n",
        "plt.figure(figsize=(8, 2))\n",
        "plt.scatter(steps, [1]*len(steps))\n",
        "\n",
        "for s, tok in zip(steps, tokens):\n",
        "    color = \"red\" if tok == \"5\" else \"black\"\n",
        "    plt.text(s, 1.02, tok, ha='center', color=color, fontsize=12)\n",
        "\n",
        "plt.yticks([])\n",
        "plt.xlabel(\"Generation Step\")\n",
        "plt.title(\"Token-Level Decision Leading to Invalid SWAT = 1.5\")\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "68b9cdab-23e7-4c34-bb13-fb38621e43e0"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.10 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}